## ì¼ë³„ ì£¼ì£¼ì´íšŒì†Œì§‘ê³µê³  ë³´ê³ ì„œ ì¡°íšŒ
## ğŸ“Œ ì£¼ì£¼ì´íšŒì†Œì§‘ê²°ì˜ ìˆ˜ì§‘ ê°€ëŠ¥ ì—¬ë¶€ í…ŒìŠ¤íŠ¸ í•„ìš”
import os


####
# * ì§„í–‰í˜„í™©
## (1) ì‚¬ì™¸ì´ì‚¬ = 2025ë…„ 3ì›” 10ì¼ ì´í›„ë¶€í„° ì—…ë°ì´íŠ¸ X
## (2) ESG = 2025ë…„ 3ì›” 14ì¼ì´í›„ë¶€í„° ì—…ë°ì´íŠ¸ X


#### 1. ì£¼ì£¼ì´íšŒì†Œì§‘ê³µê³  ë¶ˆëŸ¬ì˜¤ê¸°
# 500ëŒ€ ê¸°ì—… ì—¬ë¶€ ë° 88ê°œ ëŒ€ê·œëª¨ê¸°ì—… ì§‘ë‹¨ ê³„ì—´ì‚¬ì—ì„œ ë°œí‘œí•œ ì£¼ì£¼ì´íšŒì†Œì§‘ê³µê³  infoë§Œ ìˆ˜ì§‘

class request_DEF14A_info():
    def __init__(self):
        self.start_date = start_date
        self.end_date = end_date
        print(start_date, "ë¶€í„°", end_date,"ê¹Œì§€ ê¸°ê°„ì˜ ì£¼ì£¼ì´íšŒì†Œì§‘ê³µê³  ë³´ê³ ì„œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")

    def __call_raw_porxy_info_list__(start_date, end_date):
        """DARTì—ì„œ ê³µì‹œí•˜ëŠ” ì£¼ì£¼ì´íšŒì†Œì§‘ê³µê³ (proxy staement) ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶ˆëŸ¬ì™€ ë°˜í™˜í•˜ëŠ” ë§¤ì„œë“œ"""

        url = 'https://opendart.fss.or.kr/api/list.json' # ì¼ìë³„ íŠ¹ì • ë³´ê³ ì„œ ì—…ë¡œë“œ ëª©ë¡ API

        # ë¹ˆ df ìƒì„±(APIë¡œ ë¶ˆëŸ¬ì™€ì§€ëŠ” ë°ì´í„°ì— ë§ê²Œ ë¯¸ë¦¬ ì •ì˜ í•„ìš”)
        proxy_info = pd.DataFrame(columns = ['corp_code', 'corp_name', 'stock_code'
                                    , 'corp_cls', 'report_nm','rcept_no'
                                    , 'flr_nm', 'rcept_dt', 'rm'])

        # ë³´ê³ ì„œ í˜ì´ì§€ë³„ë¡œ ë¶ˆëŸ¬ì™€ ìŒ“ì•„ì£¼ê¸°(concat ë°©ì‹)
        page_no = 1
        while True:

            params = {"crtfc_key": api_key,
                      "bgn_de": start_date,
                      "end_de": end_date,
                      "pblntf_detail_ty": 'E006',  # ì£¼ì£¼ì´íšŒì†Œì§‘ê³µê³ 
                      "page_no": page_no,
                      "page_count": '100'
                      }


            res = requests.get(url, params=params) # API í˜¸ì¶œ
            data = res.json()   # ë¶ˆëŸ¬ì˜¨ json to dictionary ë³€í™˜

            if data['page_no'] != page_no:
                break

            page_df = pd.DataFrame(data["list"]) # dictionary to df ë³€í™˜
            proxy_info = pd.concat([proxy_info, page_df], ignore_index=True)
            page_no += 1

        return proxy_info

    def __is_vaild__(corp_code, vaild_corp_code_list):
        """ë§¤ê°œë³€ìˆ˜ corp_codeê°€ ìœ íš¨í•œ corp_codeì— í•´ë‹¹í•˜ëŠ”ì§€ í™•ì¸í•˜ì—¬ ë°˜í™˜í•˜ëŠ” ë§¤ì„œë“œ"""
        if corp_code in vaild_corp_code_list:
            return "â—"
        else:
            return ""

    def __check_vaild_corp__(proxy_info, basic_df,added_col_name):
        """basic_dfì—ëŠ” 500ëŒ€ê¸°ì—…, 88ê°œëŒ€ê·œëª¨ê¸°ì—… ì§‘ë‹¨ dfê°€ ë“¤ì–´ì™€ì•¼í•¨"""

        vaild_corp_code_list = list(basic_df['ê³ ìœ ë²ˆí˜¸'])
        proxy_info[added_col_name] =  proxy_info.apply(lambda x: __is_vaild__(x.corp_code, vaild_corp_code_list), axis=1)
        return proxy_info

    def porxy_statement_list_KR(start_date, end_date):

        raw_proxy_info = __call_raw_porxy_info_list__(start_date, end_date)

        # 500ëŒ€ê¸°ì—… ë¶ˆëŸ¬ì˜¤ê¸°
        com500_df = pd.read_excel('2024ë…„ ê¸°ì¤€ 500ëŒ€ ê¸°ì—…(ê³ ìœ ë²ˆí˜¸ ë° ì¢…ëª©ì½”ë“œ í¬í•¨).xlsx',
                            dtype={"ìˆœìœ„":int,"ì¢…ëª©ì½”ë“œ":str,"ê³ ìœ ë²ˆí˜¸":str})

        proxy_info = __check_vaild_corp__(raw_proxy_info, com500_df, '500ëŒ€ê¸°ì—… ì—¬ë¶€')

        # 88ê°œ ëŒ€ê·œëª¨ê¸°ì—…ì§‘ë‹¨ ê³„ì—´ì‚¬ ë¶ˆëŸ¬ì˜¤ê¸°
        group88_df = pd.read_excel('2024 ëŒ€ê·œëª¨ê¸°ì—…ì§‘ë‹¨ í˜„í™©(ê³ ìœ ë²ˆí˜¸ ë° ì¢…ëª©ì½”ë“œ í¬í•¨).xlsx',
                               dtype={"ì—°ë²ˆ": int,"ë²•ì¸ë“±ë¡ë²ˆí˜¸": str,"ê³ ìœ ë²ˆí˜¸": str,"ì¢…ëª©ì½”ë“œ": str})

        proxy_info = __check_vaild_corp__(proxy_info,group88_df,'ëŒ€ê·œëª¨ê¸°ì—…ì§‘ë‹¨ ì—¬ë¶€')


        proxy_info = proxy_info.loc[(proxy_info['500ëŒ€ê¸°ì—… ì—¬ë¶€'] != "") | (proxy_info['ëŒ€ê·œëª¨ê¸°ì—…ì§‘ë‹¨ ì—¬ë¶€'] != ""),]
        return proxy_info

    proxy_info = porxy_statement_list_KR(start_date = '20250310',end_date = '20250319')

file_names = []
#### 2. ì›ë³¸ë³´ê³ ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
# 2. ë³´ê³ ì„œ ì›ë³¸ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
def __save_unique_rawDEF14A__(flr_nm, rcept_no,filenames):
    """rcept_no:ê³µì‹œë³´ê³ ì„œì— ëŒ€í•œ ê³ ìœ  ë³´ê³ ì„œ ì½”ë“œ
    ë””ë ‰í† ë¦¬ ë‚´ ê³µì‹œë³´ê³ ì„œ ì›ë³¸íŒŒì¼ì„ ì €ì¥í•˜ëŠ” ë§¤ì„œë“œ"""

    # ê³µì‹œë³´ê³ ì„œ ì›ë³¸ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” API(ë°˜í™˜ ê²°ê³¼ = zip file í¬ë§·ì˜ ë°”ì´ë„ˆë¦¬ íŒŒì¼(xml))
    url = 'https://opendart.fss.or.kr/api/document.xml'

    # ê³µì‹œë³´ê³ ì„œì˜ ê³ ìœ  ì ‘ìˆ˜ë²ˆí˜¸ë¥¼ inputìœ¼ë¡œ ë„£ì–´ì¤˜ì•¼í•¨
    params = {"crtfc_key": api_key, "rcept_no": rcept_no}


    # íŒŒì¼ ì €ì¥ ê²½ë¡œ(ë””ë ‰í† ë¦¬) ì§€ì •
    sub_folder = "./ì£¼ì£¼ì´íšŒì†Œì§‘ê³µê³ ë³´ê³ ì„œ"
    doc_zip_path = os.path.abspath(f'./ì£¼ì£¼ì´íšŒì†Œì§‘ê³µê³ ë³´ê³ ì„œ/{rcept_no}.zip')
    doc_xml_path =os.path.abspath(f'./ì£¼ì£¼ì´íšŒì†Œì§‘ê³µê³ ë³´ê³ ì„œ/{rcept_no}.xml')

    if not os.path.isfile(doc_zip_path) and not os.path.isfile(doc_xml_path):
        res = requests.get(url, params=params)

        with open(doc_zip_path, 'wb') as f: # ë°”ì´ë„ˆë¦¬ ëª¨ë“œ = wb
            f.write(res.content)
    else:
        print("ì´ë¯¸ ë””ë ‰í† ë¦¬ì— ì €ì¥ëœ ê³µì‹œë¬¸ì„œì…ë‹ˆë‹¤.")
        return

    with ZipFile(doc_zip_path, 'r') as zf:
        zf.extractall(sub_folder)             # ë””ë ‰í† ë¦¬ ë‚´ zip ì••ì¶• í•´ì œ

    # íŒŒì¼ ì´ë¦„ ì €ì¥í•˜ê¸°
    zipinfo = zf.infolist()
    filenames.append([x.filename for x in zipinfo][0])

    # ì••ì¶• íŒŒì¼ ë‹«ê¸°
    zf.close()

    # ë””ë ‰í† ë¦¬ ë‚´ zip íŒŒì¼ ì§€ìš°ê¸°
    os.remove(doc_zip_path)
    print(flr_nm, "ì˜ ì£¼ì´ê³µê³  ", rcept_no,"ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
def save_unique_def14a(filenames):
    comp500_proxy_info = proxy_info.loc[proxy_info['500ëŒ€ê¸°ì—… ì—¬ë¶€'] == "â—"]
    # group88_proxy_info = proxy_info.loc[proxy_info['ëŒ€ê·œëª¨ê¸°ì—…ì§‘ë‹¨ ì—¬ë¶€'] == "â—"]


    # 500ëŒ€ ê¸°ì—…
    comp500_proxy_info.apply(lambda x: __save_unique_rawDEF14A__(x.flr_nm, x.rcept_no,filenames), axis=1)
    # group88_proxy_info.apply(lambda x: __save_unique_rawDEF14A__(x.flr_nm, x.rcept_no,filenames), axis=1)

save_unique_def14a

filename = '20250318001313.xml'

#### 3. ì›ë³¸ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# 3. ìœ„ì›íšŒ ê´€ë ¨ xml ë‚´ tableë¥¼ dfë¡œ ë§Œë“¤ê¸°
def read_xml_file(filename):
    # ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
    with open(filename, "r", encoding="utf-8") as file:
        xml_content = file.read()

    # BeautifulSoupìœ¼ë¡œ XML íŒŒì‹±
    soup = BeautifulSoup(xml_content, 'xml')

    # 'ì´ì‚¬íšŒë‚´ ìœ„ì›íšŒ'ê°€ í¬í•¨ëœ ì œëª© ì°¾ê¸°
    # ğŸ“Œ ì´í›„ì— 'ìœ„ì›íšŒ'ì™€ 'í™œë™ë‚´ì—­'ì´ ì¡´ì¬í•˜ëŠ” ì œëª© ì°¾ê¸°ë¡œ ë¦¬íŒ©í† ë§
    target_text = "ì´ì‚¬íšŒë‚´"
    for element in soup.find_all(text=lambda text: text and target_text in text):
        # print(element.parent)
        # print(f"íƒœê·¸ëª…: {element.parent.name}, ë‚´ìš©: {element.parent}")

        title_tag = element.parent  # TITLE íƒœê·¸
        table_tag = title_tag.find_next("TABLE")  # ë‹¤ìŒ TABLE íƒœê·¸ ì°¾ê¸°
        print(table_tag)


    # í…Œì´ë¸”ì´ ì¡´ì¬í•  ë•Œë§Œ ë³€í™˜
        if table_tag:
            rows = table_tag.find_all("TR")  # ëª¨ë“  <TR> íƒœê·¸ ì°¾ê¸°
            table_data = [[col.get_text(strip=True) for col in row.find_all(["TH", "TD"])] for row in rows]

            # Pandas DataFrame ìƒì„±
            df = pd.DataFrame(table_data)

            # ì¶œë ¥
            print(df)

